"use strict";(globalThis.webpackChunkphysical_ai_handbook=globalThis.webpackChunkphysical_ai_handbook||[]).push([[417],{2394:(n,e,r)=>{r.d(e,{A:()=>i});const i=r.p+"assets/images/cognitive-planning-3e7e4891a2e1dc665862f93f79bd7f95.png"},5386:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>g,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Module-04-VLA-Voice/vla-cognitive-planning","title":"VLA Cognitive Planning: LLMs to ROS 2 Actions","description":"* Understand how Large Language Models (LLMs) can enable cognitive planning for robots","source":"@site/docs/Module-04-VLA-Voice/11-vla-cognitive-planning.md","sourceDirName":"Module-04-VLA-Voice","slug":"/Module-04-VLA-Voice/vla-cognitive-planning","permalink":"/physical-ai-book/docs/Module-04-VLA-Voice/vla-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Module-04-VLA-Voice/11-vla-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"VLA Voice Action: Whisper & Voice Commands","permalink":"/physical-ai-book/docs/Module-04-VLA-Voice/vla-voice-action"},"next":{"title":"Sim-to-Real Transfer","permalink":"/physical-ai-book/docs/Advanced-Topics/sim-to-real-transfer"}}');var t=r(4848),o=r(8453);const a={},s="VLA Cognitive Planning: LLMs to ROS 2 Actions",l={},c=[{value:"Cognitive Planning Architecture",id:"cognitive-planning-architecture",level:2},{value:"LLM Integration Example",id:"llm-integration-example",level:3},{value:"Task Decomposition Strategies",id:"task-decomposition-strategies",level:2},{value:"ROS 2 Action Integration",id:"ros-2-action-integration",level:2},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Example Prompt Template",id:"example-prompt-template",level:3},{value:"Hands-on Lab",id:"hands-on-lab",level:2},{value:"Self-Assessment",id:"self-assessment",level:2}];function p(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"vla-cognitive-planning-llms-to-ros-2-actions",children:"VLA Cognitive Planning: LLMs to ROS 2 Actions"})}),"\n",(0,t.jsx)(e.admonition,{title:"Learning Objectives",type:"info",children:(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand how Large Language Models (LLMs) can enable cognitive planning for robots"}),"\n",(0,t.jsx)(e.li,{children:"Implement LLM-based task decomposition for humanoid robots"}),"\n",(0,t.jsx)(e.li,{children:"Integrate LLMs with ROS 2 action servers for complex behaviors"}),"\n",(0,t.jsx)(e.li,{children:"Design prompt engineering strategies for robotic applications"}),"\n"]})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{alt:"Hero Diagram: LLMs to ROS 2 Actions",src:r(2394).A+"",width:"688",height:"1024"})}),"\n",(0,t.jsx)(e.p,{children:"Large Language Models (LLMs) represent a breakthrough in enabling cognitive planning for robots, allowing them to understand high-level goals and decompose them into executable actions. This cognitive layer bridges natural language commands with low-level robot control, enabling more intuitive human-robot interaction."}),"\n",(0,t.jsx)(e.h2,{id:"cognitive-planning-architecture",children:"Cognitive Planning Architecture"}),"\n",(0,t.jsx)(e.p,{children:"The cognitive planning system connects LLMs with ROS 2 through multiple layers:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Natural language interface"}),": Processing high-level commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task decomposition"}),": Breaking down complex goals into subtasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action mapping"}),": Converting subtasks to ROS 2 actions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Execution monitoring"}),": Tracking progress and handling failures"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"llm-integration-example",children:"LLM Integration Example"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nimport openai\r\nimport json\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\n\r\nclass CognitivePlannerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'cognitive_planner\')\r\n\r\n        # LLM configuration\r\n        openai.api_key = "your-api-key"\r\n\r\n        # Subscribers and publishers\r\n        self.goal_sub = self.create_subscription(\r\n            String,\r\n            \'high_level_goal\',\r\n            self.goal_callback,\r\n            10\r\n        )\r\n\r\n        self.status_pub = self.create_publisher(\r\n            String,\r\n            \'planning_status\',\r\n            10\r\n        )\r\n\r\n    def goal_callback(self, msg):\r\n        """Process high-level goal using LLM"""\r\n        try:\r\n            # Use LLM to decompose the goal\r\n            plan = self.generate_plan(msg.data)\r\n\r\n            # Execute the plan\r\n            self.execute_plan(plan)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Planning error: {e}\')\r\n\r\n    def generate_plan(self, goal_text):\r\n        """Generate executable plan using LLM"""\r\n        prompt = f"""\r\n        You are a robot task planner. Decompose the following goal into specific, executable steps.\r\n        Respond in JSON format with the following structure:\r\n\r\n        {{\r\n            "steps": [\r\n                {{\r\n                    "action": "action_name",\r\n                    "parameters": {{"param1": "value1", "param2": "value2"}},\r\n                    "description": "Human-readable description"\r\n                }}\r\n            ]\r\n        }}\r\n\r\n        Goal: {goal_text}\r\n        """\r\n\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-3.5-turbo",\r\n            messages=[{"role": "user", "content": prompt}],\r\n            temperature=0.1\r\n        )\r\n\r\n        plan_json = response.choices[0].message.content\r\n        return json.loads(plan_json)\r\n\r\n    def execute_plan(self, plan):\r\n        """Execute the generated plan"""\r\n        for step in plan[\'steps\']:\r\n            self.execute_step(step)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    planner = CognitivePlannerNode()\r\n    rclpy.spin(planner)\r\n    rclpy.shutdown()\n'})}),"\n",(0,t.jsx)(e.admonition,{title:"Key Concept",type:"tip",children:(0,t.jsx)(e.p,{children:"Cognitive planning with LLMs requires careful prompt engineering to ensure the generated plans are executable by the robot and account for environmental constraints and safety requirements."})}),"\n",(0,t.jsx)(e.h2,{id:"task-decomposition-strategies",children:"Task Decomposition Strategies"}),"\n",(0,t.jsx)(e.p,{children:"LLMs can decompose complex tasks into manageable subtasks:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class TaskDecomposer:\r\n    def __init__(self):\r\n        self.knowledge_base = {\r\n            'navigation': ['move_base', 'path_planning', 'obstacle_avoidance'],\r\n            'manipulation': ['arm_control', 'grasp_planning', 'object_manipulation'],\r\n            'interaction': ['speech_synthesis', 'gesture_control', 'facial_expression']\r\n        }\r\n\r\n    def decompose_task(self, goal, context):\r\n        \"\"\"Decompose high-level goal with environmental context\"\"\"\r\n        prompt = f\"\"\"\r\n        Decompose this goal into specific robot actions considering the context:\r\n\r\n        Goal: {goal}\r\n        Context: {context}\r\n\r\n        Available capabilities: {list(self.knowledge_base.keys())}\r\n\r\n        Return JSON with executable steps.\r\n        \"\"\"\r\n\r\n        # Implementation would call LLM with this prompt\r\n        pass\n"})}),"\n",(0,t.jsx)(e.h2,{id:"ros-2-action-integration",children:"ROS 2 Action Integration"}),"\n",(0,t.jsx)(e.p,{children:"The cognitive planner must interface with ROS 2 action servers:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from rclpy.action import ActionClient\r\nfrom move_base_msgs.action import MoveBase\r\nfrom humanoid_msgs.action import ManipulateObject\r\n\r\nclass ActionExecutor:\r\n    def __init__(self, node):\r\n        self.node = node\r\n\r\n        # Action clients\r\n        self.move_base_client = ActionClient(node, MoveBase, \'move_base\')\r\n        self.manipulate_client = ActionClient(node, ManipulateObject, \'manipulate_object\')\r\n\r\n    async def execute_navigation(self, target_pose):\r\n        """Execute navigation action"""\r\n        goal_msg = MoveBase.Goal()\r\n        goal_msg.target_pose = target_pose\r\n\r\n        self.node.get_logger().info(\'Sending navigation goal...\')\r\n        future = self.move_base_client.send_goal_async(goal_msg)\r\n        result = await future\r\n        return result.result\r\n\r\n    async def execute_manipulation(self, object_name, action_type):\r\n        """Execute manipulation action"""\r\n        goal_msg = ManipulateObject.Goal()\r\n        goal_msg.object_name = object_name\r\n        goal_msg.action_type = action_type\r\n\r\n        future = self.manipulate_client.send_goal_async(goal_msg)\r\n        result = await future\r\n        return result.result\n'})}),"\n",(0,t.jsx)(e.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Effective prompt engineering is crucial for reliable cognitive planning:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Context provision"}),": Include robot capabilities and environmental constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Structured output"}),": Request specific JSON formats for easy parsing"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety constraints"}),": Include safety and ethical guidelines"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Error handling"}),": Plan for ambiguous or impossible requests"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"example-prompt-template",children:"Example Prompt Template"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def create_planning_prompt(goal, robot_capabilities, environment, safety_constraints):\r\n    return f"""\r\n    You are a cognitive planner for a humanoid robot. Plan the following task:\r\n\r\n    GOAL: {goal}\r\n\r\n    ROBOT CAPABILITIES: {robot_capabilities}\r\n    ENVIRONMENT: {environment}\r\n    SAFETY CONSTRAINTS: {safety_constraints}\r\n\r\n    INSTRUCTIONS:\r\n    1. Decompose the goal into executable steps\r\n    2. Ensure each step is achievable with the robot\'s capabilities\r\n    3. Respect all safety constraints\r\n    4. Output in the following JSON format:\r\n\r\n    {{\r\n        "plan": {{\r\n            "goal": "{goal}",\r\n            "steps": [\r\n                {{\r\n                    "id": 1,\r\n                    "action": "action_name",\r\n                    "parameters": {{"param1": "value1"}},\r\n                    "preconditions": ["condition1", "condition2"],\r\n                    "effects": ["effect1", "effect2"]\r\n                }}\r\n            ]\r\n        }}\r\n    }}\r\n\r\n    Be precise and ensure the plan is executable.\r\n    """\n'})}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-lab",children:"Hands-on Lab"}),"\n",(0,t.jsx)(e.p,{children:"Implement a cognitive planning system with:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"LLM integration for task decomposition"}),"\n",(0,t.jsx)(e.li,{children:"ROS 2 action server interface"}),"\n",(0,t.jsx)(e.li,{children:"Prompt engineering for reliable output"}),"\n",(0,t.jsx)(e.li,{children:"Plan execution and monitoring"}),"\n",(0,t.jsx)(e.li,{children:"Error handling and plan adjustment"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"self-assessment",children:"Self-Assessment"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"How do LLMs enable cognitive planning in robotics?"}),"\n",(0,t.jsx)(e.li,{children:"What are the challenges of integrating LLMs with ROS 2?"}),"\n",(0,t.jsx)(e.li,{children:"Why is prompt engineering important for robotic applications?"}),"\n"]})]})}function g(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(p,{...n})}):p(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>s});var i=r(6540);const t={},o=i.createContext(t);function a(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);