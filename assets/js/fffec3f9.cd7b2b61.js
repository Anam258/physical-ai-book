"use strict";(globalThis.webpackChunkphysical_ai_handbook=globalThis.webpackChunkphysical_ai_handbook||[]).push([[709],{3993:(e,n,r)=>{r.d(n,{A:()=>o});const o=r.p+"assets/images/VLA-voice-action-bb20084fa921c767a38f0b341aea1146.png"},4991:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"Module-04-VLA-Voice/vla-voice-action","title":"VLA Voice Action: Whisper & Voice Commands","description":"* Implement voice command recognition using Whisper and similar models","source":"@site/docs/Module-04-VLA-Voice/10-vla-voice-action.md","sourceDirName":"Module-04-VLA-Voice","slug":"/Module-04-VLA-Voice/vla-voice-action","permalink":"/physical-ai-book/docs/Module-04-VLA-Voice/vla-voice-action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Module-04-VLA-Voice/10-vla-voice-action.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim 3: Nav2 for Bipedal Movement","permalink":"/physical-ai-book/docs/Module-03-Isaac-Sim/isaac-sim-3"},"next":{"title":"VLA Cognitive Planning: LLMs to ROS 2 Actions","permalink":"/physical-ai-book/docs/Module-04-VLA-Voice/vla-cognitive-planning"}}');var i=r(4848),t=r(8453);const a={},s="VLA Voice Action: Whisper & Voice Commands",c={},l=[{value:"Voice Command Architecture",id:"voice-command-architecture",level:2},{value:"Whisper Integration Example",id:"whisper-integration-example",level:3},{value:"Natural Language Command Processing",id:"natural-language-command-processing",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"Real-time Performance Considerations",id:"real-time-performance-considerations",level:2},{value:"Hands-on Lab",id:"hands-on-lab",level:2},{value:"Self-Assessment",id:"self-assessment",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"vla-voice-action-whisper--voice-commands",children:"VLA Voice Action: Whisper & Voice Commands"})}),"\n",(0,i.jsx)(n.admonition,{title:"Learning Objectives",type:"info",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement voice command recognition using Whisper and similar models"}),"\n",(0,i.jsx)(n.li,{children:"Integrate speech-to-text capabilities with ROS 2 systems"}),"\n",(0,i.jsx)(n.li,{children:"Design voice command grammars for humanoid robot control"}),"\n",(0,i.jsx)(n.li,{children:"Process natural language commands for robot action execution"}),"\n"]})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Hero Diagram: Whisper &amp; Voice Commands",src:r(3993).A+"",width:"688",height:"1024"})}),"\n",(0,i.jsx)(n.p,{children:"Voice-based interaction represents a natural and intuitive way for humans to communicate with humanoid robots. Leveraging models like OpenAI's Whisper for speech recognition, robots can understand and execute complex voice commands, enabling seamless human-robot interaction in various environments."}),"\n",(0,i.jsx)(n.h2,{id:"voice-command-architecture",children:"Voice Command Architecture"}),"\n",(0,i.jsx)(n.p,{children:"The voice command system for humanoid robots typically involves:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech recognition"}),": Converting audio to text using models like Whisper"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural language understanding"}),": Parsing text commands into executable actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command mapping"}),": Translating understood commands to ROS 2 actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Response generation"}),": Providing feedback to the user"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"whisper-integration-example",children:"Whisper Integration Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nimport whisper\r\nimport pyaudio\r\nimport wave\r\nimport threading\r\nfrom std_msgs.msg import String\r\n\r\nclass VoiceCommandNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_node\')\r\n\r\n        # Load Whisper model\r\n        self.whisper_model = whisper.load_model("base")\r\n\r\n        # Audio recording parameters\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 44100\r\n        self.record_seconds = 5\r\n\r\n        # Publishers and subscribers\r\n        self.command_pub = self.create_publisher(String, \'voice_command\', 10)\r\n\r\n        # Start audio recording thread\r\n        self.recording = True\r\n        self.audio_thread = threading.Thread(target=self.record_audio)\r\n        self.audio_thread.start()\r\n\r\n    def record_audio(self):\r\n        """Record audio and process with Whisper"""\r\n        p = pyaudio.PyAudio()\r\n\r\n        stream = p.open(format=self.format,\r\n                        channels=self.channels,\r\n                        rate=self.rate,\r\n                        input=True,\r\n                        frames_per_buffer=self.chunk)\r\n\r\n        while self.recording:\r\n            frames = []\r\n\r\n            # Record audio\r\n            for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\r\n                data = stream.read(self.chunk)\r\n                frames.append(data)\r\n\r\n            # Save to temporary file\r\n            wf = wave.open("temp_audio.wav", \'wb\')\r\n            wf.setnchannels(self.channels)\r\n            wf.setsampwidth(p.get_sample_size(self.format))\r\n            wf.setframerate(self.rate)\r\n            wf.writeframes(b\'\'.join(frames))\r\n            wf.close()\r\n\r\n            # Process with Whisper\r\n            result = self.whisper_model.transcribe("temp_audio.wav")\r\n            text = result["text"]\r\n\r\n            # Process the recognized text\r\n            self.process_command(text)\r\n\r\n    def process_command(self, text):\r\n        """Process recognized text and publish command"""\r\n        cmd_msg = String()\r\n        cmd_msg.data = text\r\n        self.command_pub.publish(cmd_msg)\r\n        self.get_logger().info(f\'Recognized: {text}\')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    voice_node = VoiceCommandNode()\r\n\r\n    try:\r\n        rclpy.spin(voice_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        voice_node.recording = False\r\n        voice_node.audio_thread.join()\r\n        rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(n.admonition,{title:"Key Concept",type:"tip",children:(0,i.jsx)(n.p,{children:"Voice command systems for humanoid robots must handle ambient noise, multiple speakers, and real-time processing requirements while maintaining accuracy in diverse acoustic environments."})}),"\n",(0,i.jsx)(n.h2,{id:"natural-language-command-processing",children:"Natural Language Command Processing"}),"\n",(0,i.jsx)(n.p,{children:"Humanoid robots need to interpret natural language commands that may vary in structure and vocabulary:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class CommandInterpreter:\r\n    def __init__(self):\r\n        self.command_patterns = {\r\n            'move_forward': [\r\n                'move forward',\r\n                'go forward',\r\n                'walk forward',\r\n                'step forward'\r\n            ],\r\n            'turn_left': [\r\n                'turn left',\r\n                'rotate left',\r\n                'pivot left'\r\n            ],\r\n            'greet': [\r\n                'say hello',\r\n                'wave hello',\r\n                'greet me',\r\n                'hello'\r\n            ]\r\n        }\r\n\r\n    def interpret_command(self, text):\r\n        \"\"\"Interpret natural language command\"\"\"\r\n        text_lower = text.lower().strip()\r\n\r\n        for action, patterns in self.command_patterns.items():\r\n            for pattern in patterns:\r\n                if pattern in text_lower:\r\n                    return action, self.extract_parameters(text_lower, pattern)\r\n\r\n        return 'unknown', {}\r\n\r\n    def extract_parameters(self, text, pattern):\r\n        \"\"\"Extract parameters from command\"\"\"\r\n        # Example: extract distance from \"move forward 2 meters\"\r\n        import re\r\n        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(meter|metre|m)', text)\r\n        if match:\r\n            return {'distance': float(match.group(1))}\r\n        return {}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,i.jsx)(n.p,{children:"Voice commands must be integrated with ROS 2 action servers and services:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from rclpy.action import ActionClient\r\nfrom geometry_msgs.msg import Twist\r\nfrom humanoid_msgs.action import MoveBase\r\n\r\nclass VoiceController(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_controller')\r\n\r\n        # Command subscription\r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            'voice_command',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        # Action clients for robot actions\r\n        self.move_base_client = ActionClient(self, MoveBase, 'move_base')\r\n\r\n        # Command interpreter\r\n        self.interpreter = CommandInterpreter()\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Handle voice command\"\"\"\r\n        command, params = self.interpreter.interpret_command(msg.data)\r\n\r\n        if command == 'move_forward':\r\n            self.execute_move_forward(params.get('distance', 1.0))\r\n        elif command == 'turn_left':\r\n            self.execute_turn_left()\r\n        # ... handle other commands\n"})}),"\n",(0,i.jsx)(n.h2,{id:"real-time-performance-considerations",children:"Real-time Performance Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Voice command systems must balance accuracy with real-time performance:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio buffering"}),": Efficient audio capture and processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model optimization"}),": Using smaller models or quantized versions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Asynchronous processing"}),": Non-blocking command recognition"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hotword detection"}),": Wake word systems to activate listening"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-lab",children:"Hands-on Lab"}),"\n",(0,i.jsx)(n.p,{children:"Create a voice command system with:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Whisper model integration for speech recognition"}),"\n",(0,i.jsx)(n.li,{children:"Natural language command interpretation"}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 action execution based on voice commands"}),"\n",(0,i.jsx)(n.li,{children:"Audio input handling and processing"}),"\n",(0,i.jsx)(n.li,{children:"Real-time performance optimization"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"self-assessment",children:"Self-Assessment"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"How does Whisper differ from traditional speech recognition systems?"}),"\n",(0,i.jsx)(n.li,{children:"What are the challenges of voice command processing for robots?"}),"\n",(0,i.jsx)(n.li,{children:"How can natural language commands be mapped to robot actions?"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>s});var o=r(6540);const i={},t=o.createContext(i);function a(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);